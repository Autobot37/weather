============================================================
                 Experiment Start
============================================================
Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

train data: 10000, valid data: 16887, test_data: 100
Pixel Scale: 255.0, Threshold: (16, 74, 133, 160, 181, 219)
Main Model Parameters: 46.30M
============ Running training ============
    Num examples = 10000
    Num Epochs = 1
    Instantaneous batch size per GPU = 1
    Total train batch size (w. parallel, distributed & accumulation) = 1
    Total optimization steps = 10000
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
) with init lr: 0.0001
Data Loading Time: 0.12043023109436035
True
gpu_nums: 1, gpu_id: 0
sampling frags:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.45s/it]
 ========= Running Sanity Check ==========
Traceback (most recent call last):██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.45s/it]
  File "/home/vatsal/NWM/DiffCast/run.py", line 567, in <module>
    main()
  File "/home/vatsal/NWM/DiffCast/run.py", line 556, in main
    exp.train()
  File "/home/vatsal/NWM/DiffCast/run.py", line 410, in train
    self.accelerator.backward(loss_dict['total_loss'])
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/accelerate/accelerator.py", line 2001, in backward
    loss.backward(**kwargs)
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 241.94 MiB is free. Process 492339 has 584.00 MiB memory in use. Process 564616 has 16.26 GiB memory in use. Including non-PyTorch memory, this process has 6.53 GiB memory in use. Of the allocated memory 5.70 GiB is allocated by PyTorch, and 373.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
