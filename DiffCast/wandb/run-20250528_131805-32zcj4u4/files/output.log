============================================================
                 Experiment Start
============================================================
Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

train data: 10000, valid data: 16887, test_data: 100
Pixel Scale: 255.0, Threshold: (16, 74, 133, 160, 181, 219)
Main Model Parameters: 48.72M
============ Running training ============
    Num examples = 10000
    Num Epochs = 1
    Instantaneous batch size per GPU = 1
    Total train batch size (w. parallel, distributed & accumulation) = 1
    Total optimization steps = 10000
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
) with init lr: 0.0001
Data Loading Time: 0.1003420352935791
True
gpu_nums: 1, gpu_id: 0
sampling frags:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.12s/it]
backbone_net.torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_1.weight
backbone_net.torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_1.bias
backbone_net.torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_2.weight
backbone_net.torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_2.bias
 ========= Running Sanity Check ==========
Epoch 0/1, Step 160/10000:   2%|█▍                                                                                         | 161/10000 [00:47<44:44,  3.67it/s, lr=1.61e-5, total_loss=0.0922]Traceback (most recent call last):
  File "/home/vatsal/NWM/DiffCast/run.py", line 577, in <module>
    main()
  File "/home/vatsal/NWM/DiffCast/run.py", line 566, in main
    exp.train()
  File "/home/vatsal/NWM/DiffCast/run.py", line 421, in train
    self.accelerator.backward(loss_dict['total_loss'])
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/accelerate/accelerator.py", line 2001, in backward
    loss.backward(**kwargs)
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
