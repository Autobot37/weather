============================================================
                 Experiment Start
============================================================
Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

train data: 10000, valid data: 16887, test_data: 20
Pixel Scale: 255.0, Threshold: (16, 74, 133, 160, 181, 219)
Main Model Parameters: 46.30M
============ Running training ============
    Num examples = 10000
    Num Epochs = 1
    Instantaneous batch size per GPU = 1
    Total train batch size (w. parallel, distributed & accumulation) = 1
    Total optimization steps = 10000
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 0.0
    maximize: False
    weight_decay: 0.0
) with init lr: 0.0001
Data Loading Time: 0.12505459785461426
True
gpu_nums: 1, gpu_id: 0
  0%|                                                                                                                                                                       | 0/10000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/vatsal/NWM/DiffCast/run.py", line 567, in <module>
    main()
  File "/home/vatsal/NWM/DiffCast/run.py", line 556, in main
    exp.train()
  File "/home/vatsal/NWM/DiffCast/run.py", line 409, in train
    loss_dict = self._train_batch(batch)
  File "/home/vatsal/NWM/DiffCast/run.py", line 476, in _train_batch
    assert radar_batch.shape[1] == self.args.frames_out + self.args.frames_in, "radar sequence length error"
AssertionError: radar sequence length error
