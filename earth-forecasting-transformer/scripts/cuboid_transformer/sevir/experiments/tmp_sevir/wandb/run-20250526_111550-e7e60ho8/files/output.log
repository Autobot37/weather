GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 0
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

Restoring states from the checkpoint path at /home/vatsal/NWM/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/experiments/tmp_sevir/checkpoints/last.ckpt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/vatsal/NWM/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/train_cuboid_sevir.py", line 886, in <module>
[rank0]:     main()
[rank0]:   File "/home/vatsal/NWM/earth-forecasting-transformer/scripts/cuboid_transformer/sevir/train_cuboid_sevir.py", line 863, in main
[rank0]:     trainer.test(model=pl_module,
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 938, in test
[rank0]:     return self._call_and_handle_interrupt(self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
[rank0]:     return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 985, in _test_impl
[rank0]:     results = self._run(model, ckpt_path=self.ckpt_path)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1179, in _run
[rank0]:     self._restore_modules_and_callbacks(ckpt_path)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1141, in _restore_modules_and_callbacks
[rank0]:     self._checkpoint_connector.restore_model()
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 179, in restore_model
[rank0]:     self.trainer.strategy.load_model_state_dict(self._loaded_checkpoint)
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 319, in load_model_state_dict
[rank0]:     self.lightning_module.load_state_dict(checkpoint["state_dict"])
[rank0]:   File "/home/vatsal/miniconda3/envs/earthformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: Error(s) in loading state_dict for CuboidSEVIRPLModule:
[rank0]: 	Missing key(s) in state_dict: "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.ffn_1.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.ffn_1.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.ffn_2.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.ffn_2.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.layer_norm.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.0.layer_norm.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.ffn_1.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.ffn_1.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.ffn_2.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.ffn_2.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.layer_norm.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.1.layer_norm.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.ffn_1.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.ffn_1.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.ffn_2.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.ffn_2.bias", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.layer_norm.weight", "torch_nn_module.encoder.blocks.0.0.global_ffn_l.2.layer_norm.bias", "torch_nn_module.encoder.blocks.0.0.attn_l.0.global_qkv.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.global_qkv.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.global_qkv.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.ffn_1.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.ffn_1.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.ffn_2.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.ffn_2.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.layer_norm.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.0.layer_norm.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.ffn_1.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.ffn_1.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.ffn_2.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.ffn_2.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.layer_norm.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.1.layer_norm.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.ffn_1.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.ffn_1.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.ffn_2.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.ffn_2.bias", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.layer_norm.weight", "torch_nn_module.encoder.blocks.1.0.global_ffn_l.2.layer_norm.bias", "torch_nn_module.encoder.blocks.1.0.attn_l.0.global_qkv.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.global_qkv.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.global_qkv.weight", "torch_nn_module.enc_pos_embed.HW_embed.weight", "torch_nn_module.dec_pos_embed.HW_embed.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.ffn_1.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.ffn_1.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.ffn_2.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.ffn_2.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.layer_norm.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.0.layer_norm.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.ffn_1.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.ffn_1.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.ffn_2.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.ffn_2.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.layer_norm.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.1.layer_norm.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.2.ffn_1.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.2.ffn_1.bias", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.2.ffn_2.weight", "torch_nn_module.decoder.self_blocks.0.0.global_ffn_l.2.ffn_2.bias", "torch_nn_module.decoder.sel
[rank0]: 	Unexpected key(s) in state_dict: "torch_nn_module.encoder.blocks.0.0.attn_l.0.l2g_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.0.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.0.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.0.g2l_k_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.0.g2l_v_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.0.g2g_global_qkv_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.l2g_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.g2l_k_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.g2l_v_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.1.g2g_global_qkv_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.l2g_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.g2l_k_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.g2l_v_net.weight", "torch_nn_module.encoder.blocks.0.0.attn_l.2.g2g_global_qkv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.l2g_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.g2l_k_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.g2l_v_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.0.g2g_global_qkv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.l2g_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.g2l_k_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.g2l_v_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.1.g2g_global_qkv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.l2g_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.l2g_global_kv_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.g2l_global_q_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.g2l_k_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.g2l_v_net.weight", "torch_nn_module.encoder.blocks.1.0.attn_l.2.g2g_global_qkv_net.weight", "torch_nn_module.enc_pos_embed.H_embed.weight", "torch_nn_module.enc_pos_embed.W_embed.weight", "torch_nn_module.dec_pos_embed.H_embed.weight", "torch_nn_module.dec_pos_embed.W_embed.weight".
[rank0]: 	size mismatch for torch_nn_module.init_global_vectors: copying a param with shape torch.Size([8, 128]) from checkpoint, the shape in current model is torch.Size([8, 64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.0.weight: copying a param with shape torch.Size([16, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 1, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.3.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 4, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.4.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.0.4.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.0.weight: copying a param with shape torch.Size([64, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 4, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.3.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.4.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.1.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.0.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 16, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.conv_block_list.2.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.0.reduction.weight: copying a param with shape torch.Size([16, 144]) from checkpoint, the shape in current model is torch.Size([4, 36]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.0.norm.weight: copying a param with shape torch.Size([144]) from checkpoint, the shape in current model is torch.Size([36]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.0.norm.bias: copying a param with shape torch.Size([144]) from checkpoint, the shape in current model is torch.Size([36]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.1.reduction.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([16, 64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.2.reduction.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.2.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.initial_encoder.patch_merge_list.2.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.0.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.0.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.3.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.4.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.1.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.0.weight: copying a param with shape torch.Size([16, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 16, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.3.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([4, 4, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.3.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.4.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.conv_block_list.2.4.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([4]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.0.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.0.conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.1.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.1.conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.2.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.final_decoder.upsample_list.2.conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).
[rank0]: 	size mismatch for torch_nn_module.dec_final_proj.weight: copying a param with shape torch.Size([1, 16]) from checkpoint, the shape in current model is torch.Size([1, 4]).
[rank0]: 	size mismatch for torch_nn_module.encoder.down_layers.0.reduction.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([128, 256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.down_layers.0.norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.down_layers.0.norm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.down_layer_global_proj.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.down_layer_global_proj.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.0.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.1.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.ffn_l.2.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.global_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.global_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.global_vec_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.0.global_vec_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.global_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.global_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.global_vec_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.1.global_vec_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.global_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.global_proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.global_vec_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.0.0.attn_l.2.global_vec_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.ffn_1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.ffn_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.ffn_2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.ffn_2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.0.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.ffn_1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.ffn_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.ffn_2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.ffn_2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.1.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.ffn_1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.ffn_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.ffn_2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.ffn_2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.ffn_l.2.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.qkv.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.global_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.global_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.global_vec_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.0.global_vec_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.qkv.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.global_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.global_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.global_vec_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.1.global_vec_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.qkv.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.global_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.global_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.global_vec_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.encoder.blocks.1.0.attn_l.2.global_vec_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.enc_pos_embed.T_embed.weight: copying a param with shape torch.Size([13, 128]) from checkpoint, the shape in current model is torch.Size([13, 64]).
[rank0]: 	size mismatch for torch_nn_module.z_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.z_proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.dec_pos_embed.T_embed.weight: copying a param with shape torch.Size([12, 256]) from checkpoint, the shape in current model is torch.Size([12, 128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.0.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.1.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.ffn_l.2.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.0.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.0.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.0.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.0.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.0.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.1.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.1.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.1.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.1.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.1.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.2.qkv.weight: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([192, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.2.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.2.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.2.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.self_blocks.0.0.attn_l.2.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_1.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([64, 256]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.ffn_2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.layer_norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.ffn_l.0.layer_norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.q_proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.kv_proj.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.proj.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([64, 64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.proj.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.norm.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.0.0.attn_l.0.norm.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.ffn_1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.ffn_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.ffn_2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.ffn_2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.ffn_l.0.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.q_proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.kv_proj.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.proj.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.proj.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.cross_blocks.1.0.attn_l.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).
[rank0]: 	size mismatch for torch_nn_module.decoder.upsample_layers.0.conv.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).
[rank0]: 	size mismatch for torch_nn_module.decoder.upsample_layers.0.conv.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).
