{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50231f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"full_dataset\"\n",
    "ref_file = \"RCTLS_05AUG2020_161736_L2B_STD.nc\"\n",
    "\n",
    "ds_ref = convert_radartoxarray(ref_file)\n",
    "mask = build_mask(ds_ref)\n",
    "\n",
    "all_files = glob.glob(os.path.join(data_dir, \"*.nc\"))\n",
    "\n",
    "condition_window = 4\n",
    "prediction_window = 6\n",
    "batch_size = 8\n",
    "\n",
    "dataset = RadarNowcastDataset(all_files, condition_window, prediction_window, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(dataset.files[5])\n",
    "plt.figure(figsize=(10, 8))\n",
    "data = ds['DBZ'].clip(min=-30, max=70)\n",
    "print(ds['DBZ'].shape, ds['lon'].shape, ds['lat'].shape)\n",
    "plt.pcolormesh(ds['lon'], ds[\"lat\"], data[0], cmap='viridis', shading='auto')\n",
    "plt.colorbar(label=\"Reflectivity (dBZ)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bedfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size  = len(dataset) - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers = 2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers = 2, pin_memory=True)\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n",
    "for out in train_loader:\n",
    "    print(f\"Input shape: {out[\"input\"].shape}, Target shape: {out[\"target\"].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ca662",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from diffusers import UNet2DModel\n",
    "import torch.nn as nn\n",
    "from PWmodel import PWModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9986ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "CONDITION_WINDOW_SIZE = 4  \n",
    "PREDICTION_WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 1\n",
    "NUM_TRAIN_TIMESTEPS = 1000\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = PWModel(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    condition_window_size=CONDITION_WINDOW_SIZE,\n",
    "    prediction_window_size=PREDICTION_WINDOW_SIZE\n",
    ").to(device)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=NUM_TRAIN_TIMESTEPS,\n",
    "    beta_schedule=\"squaredcos_cap_v2\"\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        condition_frames = batch['input'].float().to(device) # Past frames [B, COND_W, H, W]\n",
    "        target_frames = batch['target'].float().to(device)   # Future frames [B, PRED_W, H, W]\n",
    "\n",
    "        if target_frames.shape[1] != PREDICTION_WINDOW_SIZE:\n",
    "            raise ValueError(f\"Target frames channel dimension ({target_frames.shape[1]}) \"\n",
    "                             f\"does not match PREDICTION_WINDOW_SIZE ({PREDICTION_WINDOW_SIZE}). \"\n",
    "                             \"Ensure your dataset prepares the correct number of target frames.\")\n",
    "\n",
    "        noise = torch.randn_like(target_frames) # [B, PRED_W, H, W]\n",
    "\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (target_frames.shape[0],)\n",
    "        ).long().to(device) # [B]\n",
    "\n",
    "        noisy_target_images = noise_scheduler.add_noise(target_frames, noise, timesteps) # [B, PRED_W, H, W]\n",
    "        \n",
    "        model_input = torch.cat((noisy_target_images, condition_frames), dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_noise = model(model_input, timesteps) # [B, PRED_W, H, W]\n",
    "\n",
    "        loss = loss_fn(predicted_noise, noise) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} finished. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"weights/PWmodel_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4007cbe",
   "metadata": {},
   "source": [
    "**ddpm loss 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fcfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.36 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def get_size(path):\n",
    "    if os.path.isfile(path):\n",
    "        size = os.path.getsize(path)\n",
    "    else:\n",
    "        size = sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(path) for f in files)\n",
    "    \n",
    "    power = 2**10\n",
    "    i = 0\n",
    "    units = ['B', 'KB', 'MB', 'GB', 'TB']\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        i += 1\n",
    "    return f\"{size:.2f} {units[i]}\"\n",
    "\n",
    "print(get_size(\"weights/vae_epoch_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_frames(sample, vmin=0, vmax=70, label1=\"Input Frame\", label2=\"Target Frame\", save = False, name = None):\n",
    "    x = sample['input'].numpy()  # shape: [COND_W, H, W]\n",
    "    y = sample['target'].numpy() # shape: [PRED_W, H, W]\n",
    "    lat = sample['lat'].numpy()\n",
    "    lon = sample['lon'].numpy()\n",
    "    H, W = lat.shape\n",
    "    pad_h = x.shape[1] - H\n",
    "    pad_w = x.shape[2] - W\n",
    "    top = pad_h // 2\n",
    "    left = pad_w // 2\n",
    "\n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(4 * max(n_x, n_y), 4 * 2))\n",
    "\n",
    "    # Plot all input frames\n",
    "    for i in range(n_x):\n",
    "        x_cropped = x[i, top:top+H, left:left+W]\n",
    "        x_cropped = (x_cropped + 1.0) / 2.0 * 100 - 30\n",
    "        plt.subplot(2, max(n_x, n_y), i + 1)\n",
    "        plt.pcolormesh(lon, lat, x_cropped, cmap='Blues', vmin=vmin, vmax=vmax)\n",
    "        plt.title(f\"{label1} {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Plot all target frames\n",
    "    for i in range(n_y):\n",
    "        y_cropped = y[i, top:top+H, left:left+W]\n",
    "        y_cropped = (y_cropped + 1.0) / 2.0 * 100 - 30\n",
    "        plt.subplot(2, max(n_x, n_y), max(n_x, n_y) + i + 1)\n",
    "        plt.pcolormesh(lon, lat, y_cropped, cmap='Blues', vmin=vmin, vmax=vmax)\n",
    "        plt.title(f\"{label2} {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(f\"plots/{name}.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_image_batch(\n",
    "    condition_images: torch.Tensor, #[B, COND_W, H, W]\n",
    "    model: torch.nn.Module,       \n",
    "    noise_scheduler,                \n",
    "    device: torch.device,\n",
    "    num_inference_steps: int = 50,\n",
    "    clamp_output: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates a batch of future frame sequences using the diffusion model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = condition_images.shape[0]\n",
    "    image_height = condition_images.shape[2] \n",
    "    image_width = condition_images.shape[3]  \n",
    "\n",
    "    pred_w = 1\n",
    "\n",
    "    # Initial noisy image (target) for the diffusion process\n",
    "    # Shape: [B, PRED_W, H, W]\n",
    "    generated_images = torch.randn(\n",
    "        (batch_size, pred_w, image_height, image_width),\n",
    "        device=device\n",
    "    )\n",
    "    noise_scheduler.set_timesteps(num_inference_steps)\n",
    "    for t in tqdm(noise_scheduler.timesteps, desc=\"Batch Inference Step\", leave=False):\n",
    "        # model_input: [B, PRED_W + COND_W, H, W]\n",
    "        model_input = torch.cat((generated_images, condition_images), dim=1)\n",
    "        noise_pred = model(model_input, t).sample # Shape: [B, PRED_W, H, W]\n",
    "        #previous noisy sample x_t -> x_t-1\n",
    "        generated_images = noise_scheduler.step(noise_pred, t, generated_images).prev_sample\n",
    "    if clamp_output:\n",
    "        generated_images = torch.clamp(generated_images, -1.0, 1.0)\n",
    "    return generated_images # Shape: [B, PRED_W, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"weights/PWmodel_epoch_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(test_loader))\n",
    "condition_frames = out['input'].float().to(device) # Past frames [B, COND_W, H, W]\n",
    "target_frames = out['target'].float().to(device)   # Future frames [B, PRED_W, H, W]\n",
    "\n",
    "TILL = 4\n",
    "\n",
    "for i in range(TILL):\n",
    "    generate_images = generate_image_batch(\n",
    "        condition_frames,\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        device,\n",
    "        num_inference_steps=500,\n",
    "        clamp_output=False\n",
    "    )\n",
    "    for batch in range(generate_images.shape[0]):\n",
    "        sample = {\n",
    "            'input': target_frames[batch].cpu(),\n",
    "            'target': generate_images[batch].cpu(),\n",
    "            'lat': out['lat'][batch],\n",
    "            'lon': out['lon'][batch]\n",
    "        }\n",
    "        plot_all_frames(sample, label1=f\"batch {batch} truth Frame\", label2=f\"batch {batch}Generated Frame\")\n",
    "    new_condition = torch.cat([\n",
    "        condition_frames[:, 1:],  \n",
    "        generate_images[:, :1]    \n",
    "    ], dim=1)\n",
    "    dummy_images = torch.zeros_like(generate_images)\n",
    "    new_target = torch.cat([\n",
    "        target_frames[:, 1:],    \n",
    "        dummy_images[:, :1]     \n",
    "    ], dim=1)\n",
    "    \n",
    "    condition_frames = new_condition\n",
    "    target_frames = new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "len(os.listdir(\"full_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 19GB \n",
    "model = UNet2DModel(\n",
    "    sample_size=512,  # the target image resolution\n",
    "    in_channels=1 + 4,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(32, 32, 64, 64, 128, 128),    \n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "NUM_TRAIN_TIMESTEPS = 1000\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda\"\n",
    "print(\"Using device:\", device)\n",
    "model.load_state_dict(torch.load(\"weights/diffusion_only_ddpm9.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "NUM_TRAIN_TIMESTEPS = 1000\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS)\n",
    "n_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda:1\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "torch.compile(model)\n",
    "loss_fn = nn.MSELoss()\n",
    "model.train()\n",
    "print(\"Training model...\")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in range(n_epochs):\n",
    "    for out in tqdm(train_loader):\n",
    "        condition = out[\"input\"].float() # [B, window_size, 512, 512]\n",
    "        target = out[\"target\"].float()  # [B, 1, 512, 512] \n",
    "        condition = condition.to(device) \n",
    "        target = target.to(device)\n",
    "        \n",
    "        noise = torch.randn_like(target) # [B, 1, 512, 512]\n",
    "        timesteps = torch.randint(0, NUM_TRAIN_TIMESTEPS, (target.shape[0],)).long().to(device) # [B]\n",
    "        noisy_target_images = noise_scheduler.add_noise(target, noise, timesteps).to(device) # [B, 1, 512, 512]\n",
    "        model_input = torch.cat((noisy_target_images, condition), dim=1) #[B, 1+window_size, 512, 512]\n",
    "        pred = model(model_input, timesteps).sample # [B, 1, 512, 512]\n",
    "        loss = loss_fn(pred, noise)  # [B, 1, 512, 512] - [1, 1, 512, 512]\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    torch.save(model.state_dict(), f\"weights/diffusion_only_ddpm-30{epoch}.pth\")\n",
    "\n",
    "    avg_loss = sum(losses[-100:]) / 100\n",
    "    print(f\"Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ed9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"weights/diffusion_only_ddpm9.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa222d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_trajectory(model, noise_scheduler, sample_batch, device, TILL=4):\n",
    "    condition_frames = sample_batch['input'].float().to(device)#[B, COND_W, H, W]\n",
    "    target_frames = sample_batch['target'].float().to(device)#[B, PRED_W, H, W]\n",
    "    lat = sample_batch['lat'].cpu()#[B, H, W]\n",
    "    lon = sample_batch['lon'].cpu()#[B, H, W]\n",
    "    condition_frames = condition_frames[0].unsqueeze(0)  # [1, COND_W, H, W]\n",
    "    target_frames = target_frames[0].unsqueeze(0)  # [1, PRED_W, H, W]\n",
    "\n",
    "    print(lat.shape, lon.shape)\n",
    "    for step in range(TILL):\n",
    "        generated = generate_image_batch(\n",
    "            condition_frames,\n",
    "            model,\n",
    "            noise_scheduler,\n",
    "            device,\n",
    "            num_inference_steps=1000,\n",
    "            clamp_output=False\n",
    "        )# [1, 1, H, W]\n",
    "        plot_all_frames(\n",
    "            {\n",
    "                'input': target_frames[0][step + 1:step + 2].cpu(),\n",
    "                'target': generated[0].cpu(),\n",
    "                'lat': lat[0],\n",
    "                'lon': lon[0]\n",
    "            },\n",
    "            label1=f\"Truth Frame\",\n",
    "            label2=f\"Generated Frame\",\n",
    "            save=True,\n",
    "            name=f\"ddpm trajectory step_{step + 1}\"\n",
    "        )\n",
    "        condition_frames = torch.cat([\n",
    "            condition_frames[:, 1:],  # remove the first frame\n",
    "            generated[:, :1]         # add the generated frame\n",
    "        ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(test_loader))\n",
    "\n",
    "generate_trajectory(\n",
    "    model, \n",
    "    noise_scheduler, \n",
    "    out,\n",
    "    device,\n",
    "    TILL=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d19688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
